{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python version of [Reliability Analysis in R](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/). The aim of this jupyter notebook is to recreate that reliability analysis in python, and therefore many of the results here are compared directly to those in the R script. Most of the explanations have also been adapted. \n",
    "\n",
    "This script provides a demonstration of some tools that can be used to conduct a reliability analysis in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, you will need the following python packages installed: [`numpy`](https://numpy.org/), [`matplotlib`](https://matplotlib.org/), [`pandas`](https://pandas.pydata.org/), [`reliability`](https://pypi.org/project/reliability/), [`weibull`](https://pypi.org/project/weibull/),  [`statsmodels`](https://pypi.org/project/statsmodels/), [`scipy`](https://www.scipy.org/) and [`math`](https://docs.python.org/3/library/math.html). Note that the weibull package is being superceded by reliability, which is currently in development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. This example analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example script the aim is to demonstrate how reliability analyses can be conducted in R and thus present only some of the analytical tools available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented is an example analysis of a dataset of time-to-failure measurements that includes censored measurements $(n=127)$. Censoring is the process of coding usage (e.g., time) measurements taken where failures have not occurred. For instance, a right-censored observation (or suspension) may occur because the item is still functioning at the time of measurement.\n",
    "\n",
    "These data represent failures of Ground Engaging Tools (GET) used in hard rock mining excavators. This dataset comes from a single excavator over an 8 year period.\n",
    "\n",
    "Ground engaging tools attach to the excavator bucket, they are also known as “teeth” due to their similarity in shape to human teeth. There are usually between 5 and 8 teeth on a bucket. GET are designed to wear out and are replaced when they functionally fail. This functional failure is defined by the ability of the tooth to penetrate the rock and power required. Each failure/ suspension event in the data represents the replacement of one or more teeth. The records do not state how many teeth are replaced at each event.\n",
    "\n",
    "These data were collected as part a mobile mining equipment database described in Ho (2016). Details on the data collection, cleaning and processing are described in Hodkiewicz and Ho (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in estimating some properties of the reliability of a system, or of a component of a system, including predicting the mean time-to-failure (MTTF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity here we assume the distribution of time-to-failure measurements are well approximated by the 2-parameter Weibull distribution. The validity of this assumption, however, is arguable (see below) and we do not recommend making this assumption in all cases. Accordingly, we also present some tools for evaluating the suitability of some alternative parametric distributions (following Meeker & Escobar 1998).\n",
    "\n",
    "The probability density function $f(t)$ of the 2-parameter Weibull distribution is defined for the random variable $T$, which is the lifetime of a component or system (i.e., where $P[T=t]=f(t)$):\n",
    "\n",
    "<center>$f(t) = \\frac{\\beta}{\\eta}(\\frac{t}{\\eta})^{\\beta -1}e^{-(\\frac{t}{\\eta})^{\\beta}}$</center>\n",
    "\n",
    "For reliability analysis, $t$ is some measure of usage (e.g., total operating or running time) and $\\beta, \\eta$ are “shape” and “scale” parameters, and each can also be interpreted as having a specific meaning (see below).\n",
    "\n",
    "We use maximum likelihood estimation ( _MLE_ ) to estimate these parameters of the Weibull distribution $(\\hat{\\beta}, \\hat{\\eta})$. This, in turn, allows us to estimate the expectation, or centre of mass, of the Weibull probability density function ( _PDF_ ) for $T$, $E[T]$. The estimate of _MTTF_ is then taken as the $E[T]$, as predicted from, and contingent upon the assumptions of, this analysis model. An alternative method for estimating these parameters is from using Median Ranked Regression ( _MRR_ ; e.g., see Abernethy 2003, O’Connor and Kleyner 2012). A recent study by Genschel and Meeker (2010) demonstrated that, for most datasets, MLE was likely to produce more reliable estimates of Weibull parameters than MRR, and that this was consistent with evidence from several other independently published studies. Please refer to Genschel and Meeker (2010) for further details.\n",
    "\n",
    "We caution, however, that one should always assess appropriate diagnostics to evaluate the validity of model assumptions for every dataset analysed, and acknowledge sources of uncertainty in every new set of results (e.g., see Meeker and Escobar 1998)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load required packages, functions and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import the `numpy`, `matplotlib.pyplot`, `pandas`, `reliability`, `weibull `, `statsmodels.distributions.empirical_distribution` , `scipy.stats` and `math` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib scipy pandas reliability weibull statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import reliability as rb\n",
    "import weibull as wb\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import scipy.stats as st\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further information on each package you can type commands such as `help(wb)`.\n",
    "\n",
    "Next, we need to load the dataset. Here, we use the `pandas` function _read_csv_ to load the _comma separated values_ (csv) file in as a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exa1 = pd.read_csv('https://raw.githubusercontent.com/CodeOwl94/ross-reliability/master/EXA1.csv',\n",
    "                   header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A csv file can be prepared by saving these data, once appropriately formatted (see below) in an MS Excel worksheet, as this type of file using the _“Save As”_ option, as selected from the _“File”_ menu in Excel. The _header=0_ argument is specified as we wish to retain the appropriately labelled data columns within the data frame object.\n",
    "\n",
    "**NOTE**: If, after stepping through this example script, you wish to apply these functions to analyse your own dataset, you will need to firstly reformat your data into a .csv file, and then load it into Python, as demonstrated above. However you will need to enter your file name as the first argument of the `pd.read_csv()` function instead. There are other ways of importing data into Python, but if you need more information about this method, type the command `help(pd.read_csv)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(exa1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data appears to have been read in OK, we know this as the number of rows, 127, and number of columns, 2, match both the <a href=\"https://raw.githubusercontent.com/CodeOwl94/ross-reliability/master/EXA1.csv\">original data set</a> and the <a href=\"https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/\"> R script</a>. \n",
    "\n",
    "Always check this, as errors may occur in this step, such as missing values or columns.\n",
    "\n",
    "Now, lets have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exa1['fail'].value_counts())\n",
    "\n",
    "exa1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing this file, we coded the values in the `fail` column as:\n",
    "\n",
    "- F, _Failure:_ a valid time-to-failure measurement, or\n",
    "- S, _Suspension:_ a right censored measurement.\n",
    "\n",
    "None of the packages used require binary coding for `fail` data, however for consistency we will recode the `fail` values such that:\n",
    "\n",
    "- 1, _Failure:_ a valid time-to-failure measurement, or\n",
    "- 0, _Suspension:_ a right censored measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa1.rename(columns={'fail':'event'}, inplace=True)\n",
    "\n",
    "exa1.event.replace(to_replace='F', value=1, inplace=True)\n",
    "exa1.event.replace(to_replace='S', value=0, inplace=True)\n",
    "\n",
    "print(exa1['event'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first sort the data by `time`, in ascending order to make it easier to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa1 = exa1.sort_values(by='time',ignore_index=True, ascending=True)\n",
    "exa1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create the following dataframes and lists:\n",
    "- `failures`: A pandas dataframe containing only valid time-to-failure measurements.\n",
    "- `right_censored`: A pandas dataframe containing only right censored measurements.\n",
    "- `failure_times`: List of time-to-failure measurements in ascending order\n",
    "- `right_censored_times`: List of right censored measurements in ascending order\n",
    "\n",
    "We will use these later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = exa1[exa1['event']==1]\n",
    "right_censored = exa1[exa1['event']==0]\n",
    "\n",
    "failure_times = failures['time'].values.tolist()\n",
    "right_censored_times = right_censored['time'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll first sort the data by `time`, in ascending order to make it easier to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exa1 = exa1.sort_values(by='time',ignore_index=True, ascending=True)\n",
    "exa1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Graphical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `matplotlib` we can have an initial look at the data. Importantly, this step helps to understand these data prior to fitting any models or making strong model assumptions (Meeker and Escobar, 1998). Results will inform initial decisions made during subsequent model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis = np.arange(0, len(exa1), 6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mask1 = exa1['event']==1\n",
    "mask2 = exa1['event']==0\n",
    "\n",
    "ax.barh(np.arange(0,len(exa1))[mask1], exa1['time'][mask1], color='black', label='Failure')\n",
    "ax.barh(np.arange(0,len(exa1))[mask2], exa1['time'][mask2], color='red', label='Suspension')\n",
    "\n",
    "ax.set_yticks(yaxis)\n",
    "ax.set_yticklabels(yaxis)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Time-ranked observation')\n",
    "ax.set_title('Figure 1.1')\n",
    "ax.legend()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the sorted raw failure times, with censored time coloured in red. Note that this differs from the [R script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/) in that these data are ordered from low time measurement at the bottom to high time measurement at the top, compared to the inverse in the R script.\n",
    "\n",
    "\n",
    "This plot is a little difficult to see, so instead we'll look at the first 60 observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the observations ranked 1 to 60, in terms of time measurement we can see the first right-censored measurement occurs at rank 48, along with three other measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis = np.arange(0, len(exa1[:60]), 2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mask1 = exa1[:60]['event']==1\n",
    "mask2 = exa1[:60]['event']==0\n",
    "\n",
    "ax.barh(np.arange(0,len(exa1[:60]))[mask1], exa1[:60]['time'][mask1], \n",
    "        color='black', label='Failure')\n",
    "ax.barh(np.arange(0,len(exa1[:60]))[mask2], exa1[:60]['time'][mask2], \n",
    "        color='red', label='Suspension')\n",
    "\n",
    "ax.set_yticks(yaxis)\n",
    "ax.set_yticklabels(yaxis)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Time-ranked observation')\n",
    "ax.set_title('Figure 2.0')\n",
    "ax.legend()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first right-censored measurement occurs at the 48th observation (which is tied with 3 other measurements).\n",
    "\n",
    "Looking at the remaining time measurements (longer operating times), we see that the three other censored measurements occur at ranks 75,  102, and 123.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis = np.arange(60, len(exa1), 2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mask1 = exa1[60:]['event']==1\n",
    "mask2 = exa1[60:]['event']==0\n",
    "\n",
    "ax.barh(np.arange(60,len(exa1))[mask1], exa1[60:]['time'][mask1], \n",
    "        color='black', label='Failure')\n",
    "ax.barh(np.arange(60,len(exa1))[mask2], exa1[60:]['time'][mask2], \n",
    "        color='red', label='Suspension')\n",
    "\n",
    "ax.set_yticks(yaxis)\n",
    "ax.set_yticklabels(yaxis)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Time-ranked observation')\n",
    "ax.set_title('Figure 3.0')\n",
    "ax.legend()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to verify the ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(exa1[exa1['event']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above plots we use the following commands:\n",
    "- `yaxis = np.arange({start}, len(exa1), {step})` are the values we want for the 'ticks' on the y-axis, such that the values are spaced enough that we can see them.\n",
    "- `mask{1, 2} = exa1[{start}:{end}]['event']=={1,0}` is so that we can colour the bars based on whether the event value is a failure (1) or suspension (0).\n",
    "- `ax.set_yticks(yaxis)` sets the ticks on the y-axis to the values specified above. \n",
    "- `ax.set_yticklabels(yaxis)` sets the labels of the ticks on the y-axis to the values specified above.\n",
    "\n",
    "Where {} is a user specified value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll have a look at the empirical cumulative frequency distribution of the complete time-to-failure measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecdf = ECDF(failures['time'])\n",
    "\n",
    "qtl_25 = failures.quantile(q = 0.25)\n",
    "qtl_5 = failures.quantile(q = 0.5)\n",
    "qtl_75 = failures.quantile(q = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(ecdf.x, ecdf.y, '-k')\n",
    "plt.plot(ecdf.x, ecdf.y, 'ok')\n",
    "\n",
    "### Horizontal Lines at 0 and 1\n",
    "plt.plot([0, max(failures['time'])], [0,0], '--y')\n",
    "plt.plot([0, max(failures['time'])], [1,1], '--y')\n",
    "\n",
    "### Quantile Lines\n",
    "plt.plot([qtl_25, qtl_25], [0, 1], '--r')\n",
    "plt.plot([qtl_5, qtl_5], [0, 1], '--r')\n",
    "plt.plot([qtl_75, qtl_75], [0, 1], '--r')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Fn(x)')\n",
    "ax.set_title('Figure 4.0')\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we adjust these relative frequencies for the censored observations to obtain non-parametric estimates of the probability of failure with time, $F(t)$.\n",
    "\n",
    "Next we can inspect the sampled frequencies as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "bins = np.arange(0, max(failures['time'])+10, 10)\n",
    "plt.hist(failures['time'], bins=bins, range=(5, max(failures['time'])), \n",
    "         color='grey', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Time (non-censored measurements)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Figure 5.0')\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right-skewed distributions are typical of reliability data. Are these data from a single distribution? If there are unusual (e.g. multi-modal) patterns it might be worth seeking additional information about how these data were sampled. \n",
    "\n",
    "To return the plotted values, we first create a `bin` column in our data frame, and then we can look at the frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, max(failures['time'])+10, 10)\n",
    "\n",
    "failures['binned'] = pd.cut(failures['time'], bins)\n",
    "failures['binned'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed counts shows the sampled frequencies of failure time measurements (column 2) alongside the bin values.\n",
    "\n",
    "These counts show that the majority of failures (i.e., approximately 85% of non-censored measurements) occurred prior to time 30, with small sample sizes for bins thereafter (and especially after time 60), which should be borne in mind when interpretting results.\n",
    "\n",
    "Next we construct probability plots, as suggested in Meeker and Escobar (1998), to identify which distribution(s) may provide a good approximation of these data. We can use the `Fit_Everything` function from the reliability package. This function returns parameter estimates, a plot of the CDF and PDF of a distribution against the histogram of failure data, and probability plots. For now, we'll just look at the probability plots and parameter estimates.\n",
    "\n",
    "Note that the library is in active development and so does not yet have confidence intervals, but we can still however get an initial idea of which distributions the data may follow based on how 'well' they overlay the fitted line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Figure 6\")\n",
    "results = rb.Fitters.Fit_Everything(failures=failure_times, \n",
    "                                    right_censored=right_censored_times, \n",
    "                                    show_histogram_plot=False, show_PP_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the <a href=\"https://reliability.readthedocs.io/en/latest/Probability%20plots.html#what-does-a-probability-plot-show-me\">Reliability docs</a>:\n",
    "\n",
    "> These probability plots show how well the data is modelled by a particular distribution. The axes are scaled in such a way that the fitted distribution's CDF appears to be a straight line. The plots can be interpreted as:\n",
    "- The y-axis is the unreliability, $\\hat{F}(t)$.\n",
    "- The x-axis is time, $t$.\n",
    "- If the empirical CDF of the failure data (the black dots) lie on the straight line then the distribution is a good fit for the data.\n",
    "- We usually tolerate a little bit of deviation at the tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the presented probability plots:\n",
    "The plotted $\\hat{F}(t)$ against $t$ on the Normal scale are clearly non-linear, which suggests that this distribution is likely to provide a poor description of the sampled times.  The plotted points on the 2 and 3 parameter Weibull, 2 and 3 parameter Lognormal, 1 and 2 parameter Exponential and 2 and 3 parameter Gamma scales look fairly linear, \n",
    "\n",
    "It could, however, be argued that times earlier than 7 units may not be consistent with later times, as approximated by the 2-parameter Weibull. This may also suggest a case for investigating the fit of the 3-parameter Weibull model to these data.\n",
    "\n",
    "As there is no ability to plot the confidence interval, we cannot comment on the level of uncertainty of these distributions from these plots.\n",
    "\n",
    "We can, however, use the AIC and BIC goodness of fit values to help select a distribution.\n",
    "\n",
    "We can have a closer look at any of these plots by plotting them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from reliability.Probability_plotting import Weibull_probability_plot\n",
    "\n",
    "Weibull_probability_plot(failures=failure_times)\n",
    "\n",
    "plt.title('Figure 7 - Weibull Distribution')\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Fit models and estimate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fit the Weibull model using the `reliability` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbfit = rb.Fitters.Fit_Weibull_2P(failures=failure_times, \n",
    "                                   right_censored=right_censored_times)\n",
    "plt.title(\"Figure 8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the same probability plot and parameter estimations as before, but also includes estimates for the confidence intervals of the parameters.\n",
    "\n",
    "The estimated parameters by this model are parameters $\\hat{\\alpha}$ (*$\\hat{\\eta}$ in the [R script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/)*) and $\\hat{\\beta}$ (*$\\hat{\\beta}$ in the [R script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/)*).\n",
    "\n",
    "Our estimates for these parameters are very similar to what was obtained in the [R script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/)).\n",
    "\n",
    "\n",
    "And how well does this fit the data? The `Fit_Weibull_2P` function already prints the plot when called, so we can visualise it above. We can see in that plot that beyond time 7 (approx.), the line fits the data relatively well. We may, however, achieve better results with a different distribution (Lognormal or Exponential).\n",
    "\n",
    "\n",
    "For further information about this function type `help(rb.Fitters.Fit_Weibull_2P)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: I have yet to find a function that plots the joint uncertainty of the estimated Weibull model parameters as in the [R script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/) (Figure 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn about the reliability of this component or system from this fitted model?\n",
    "\n",
    "Well, firstly, the value of $\\hat{\\beta}$= 1.12 is near 1, and the approximate 95% confidence interval for $\\hat{\\beta}$ of $[0.99, 1.12]$ contains 1, which suggests that there is neither evidence for failures predominantly occurring due to ageing or wearout effects (i.e., $\\hat{\\beta}$ is not high), nor due to early failures, such as may occur due to manufacturing defects (i.e., $\\hat{\\beta}$ is not low)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll plot the failure rate, or *hazard* function, $h(t)$,  which describes the likelihood of failure during the next time increment $(\\text{i.e., } \\frac{dR}{dt}=-h(t)R(t))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.arange(0, max(failure_times))\n",
    "\n",
    "rb.Distributions.Weibull_Distribution(alpha=wbfit.alpha, beta=wbfit.beta).HF(xvals=xvals)\n",
    "plt.title(\"Figure 9\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Failure Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the $\\hat{\\beta}$ is only slightly above 1 and below 2, we knew that there was not a large increase in the failure rate with increasing time. A steeper increase in the hazard function occurs earlier, than later, in the life of this component or system (given model assumptions).\n",
    "\n",
    "Next, we plot the Reliability function $R(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.arange(0, max(exa1['time']))\n",
    "\n",
    "survival = rb.Distributions.Weibull_Distribution(alpha=wbfit.alpha, \n",
    "                                                 beta=wbfit.beta).SF(xvals=xvals)\n",
    "\n",
    "plt.plot([30, 30], [0, 1], '--r')\n",
    "\n",
    "plt.title(\"Figure 10\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Reliability\")\n",
    "plt.show()\n",
    "\n",
    "print('Reliability at time t=30:', round(survival[30],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the trend of Reliability (the probability that the component or system is still functioning at time $t$) with $t$, also know as the Survival Function.\n",
    "\n",
    "As the `Weibull_Distribution` function returns an array of 'reliability' from *time=0* to a specified time, we can simply index that array at element 30 to find the value.\n",
    "\n",
    "Next, we'll calculate the MTTF from the $\\mathbb{E}[T]$ using $\\hat{\\beta}$ and $\\hat{\\eta}$. The `reliability` package doesn't have this feature yet, so we'll use the old `weibull` package.\n",
    "\n",
    "We'll do this on all of our data, so we'll create a boolean mask for failure(True) and suspension (False), and a list of all the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bools = [event == 0 for event in exa1['event']]\n",
    "times = exa1['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = wb.Analysis(data=times, suspended=bools)\n",
    "\n",
    "analysis.fit()\n",
    "\n",
    "analysis.probplot()\n",
    "\n",
    "print(\"MTTF = \", round(analysis.mttf,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this value to the [R Script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/) we can see that the `weibull` package is poor at fitting a Weibull distribution to this data set. We can however, use the parameter estimatations obtained from the `Fit_Weibull_2P` function to better estimate the *MTTF*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = wb.Analysis(data=times, suspended=bools)\n",
    "\n",
    "analysis.fit()\n",
    "\n",
    "analysis.beta = wbfit.beta\n",
    "analysis.eta = wbfit.alpha\n",
    "\n",
    "analysis.probplot()\n",
    "\n",
    "print(\"MTTF = \", round(analysis.mttf, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can see is close to the value obtained in the [R Script](https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/).\n",
    "\n",
    "In order to calculate the 95% Confidence intervals for MTTF we can obtain relatively accurate estimates using a computer-intensive method known as bootstrapping. On a standard laptop this may take 10-15 mins to run, so you may want to get a cup of tea before running …\n",
    "\n",
    "There is no function to do bootstrapping in the `weibull` or `reliability` packages, so we'll bootstrap doing the following:\n",
    "1. Sample with replacement from the original data set\n",
    "2. Generate the values for $\\hat{\\beta}$ and $\\hat{\\eta}$ using the `reliability` package\n",
    "3. Generate the MTTF value using the `weibull` package\n",
    "4. Calculate the statistics for the MTTF\n",
    "\n",
    "**Note**: the warning \"the maximum likelihood method is likely to yield better results with {} data points\" is automatic and I haven't found a solution to turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##For reproducibility\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mttf_vals = []\n",
    "\n",
    "false_array = np.full(len(failure_times),fill_value=False,dtype=bool)\n",
    "true_array = np.full(len(right_censored_times),fill_value=True,dtype=bool)\n",
    "bools = np.concatenate((false_array, true_array)).tolist()\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    new_failure_times = np.random.choice(failure_times, size=len(failure_times))\n",
    "    new_right_times = np.random.choice(right_censored_times, size=len(right_censored_times))\n",
    "    \n",
    "    new_times = np.concatenate((new_failure_times, new_right_times)).tolist()\n",
    "    \n",
    "    parameters = rb.Fitters.Fit_Weibull_2P(failures=new_failure_times, \n",
    "                                           right_censored=new_right_times,\n",
    "                                           show_probability_plot=False, \n",
    "                                           print_results=False)\n",
    "    wb.Analysis(data=new_times, suspended=bools)\n",
    "    analysis.fit()\n",
    "    analysis.beta = parameters.beta\n",
    "    analysis.eta = parameters.alpha\n",
    "    \n",
    "    mttf_vals.append(analysis.mttf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ci = st.t.interval(0.95,df=len(mttf_vals)-1, loc=st.describe(mttf_vals).mean, \n",
    "                   scale = sqrt(st.describe(mttf_vals).variance))\n",
    "\n",
    "print('minimum: ', round(st.describe(mttf_vals).minmax[0]),3)\n",
    "print('lower 95% CI:', round(ci[0],3))\n",
    "print('mean: ', round(st.describe(mttf_vals).mean,3))\n",
    "print('upper 95% CI:', round(ci[1],3))\n",
    "print('maximum: ', round(st.describe(mttf_vals).minmax[1],3))\n",
    "print('variance: ', round(st.describe(mttf_vals).variance,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the MTTF to an empirical estimate for the non-censored failures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(round(np.mean(failures['time']),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That this value is close to the Weibull MLE for $\\mathbb{E}[T]$ likely reflects that there was a relatively small proportion of censored measurements in this dataset. \n",
    "\n",
    "\n",
    "Let's plot the estimated Weibull pdf of t, with $\\mathbb{E}[T]$ superimposed as a vertical solid line, with the approximate 95% confidence bounds for $\\mathbb{E}[T]$ shown as dashed lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.arange(0, max(failure_times))\n",
    "\n",
    "wbfit = rb.Fitters.Fit_Weibull_2P(failures=failure_times, right_censored=right_censored_times,\n",
    "                                   show_probability_plot=False, print_results=False)\n",
    "pdf = rb.Distributions.Weibull_Distribution(alpha=wbfit.alpha, beta=wbfit.beta).PDF(xvals=xvals)\n",
    "\n",
    "plt.plot([st.describe(mttf_vals).mean, st.describe(mttf_vals).mean], \n",
    "         [0, pdf[int(st.describe(mttf_vals).mean)]], '-r')\n",
    "plt.plot([ci[0],ci[0]],[0, pdf[int(ci[0])] ], '--r')\n",
    "plt.plot([ci[1],ci[1]],[0, pdf[int(ci[1])] ], '--r')\n",
    "\n",
    "\n",
    "plt.title(\"Figure 13\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"f(t)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to Figure 5, a histogram of the non-censored time measurements. As it looks similar, and given the relatively small proportion of censored measurements in this dataset, this is further evidence to infer that this model has adequately captured the properties of this dataset. However, more formal tests could be conducted (see Meeker and Escobar, 1998). This also demonstrates that the Weibull distribution is quite flexible in that it can approximate the Exponential distribution (when $\\beta=1$) as well as right-skewed and symmetric unimodal distributions. We can also see that the approximate 95% confidence interval for $E[T]$, our estimate of mean time to failure, is non-symmetrical, reflecting the skewed distribution of failure times.\n",
    "\n",
    "---\n",
    "\n",
    "Importantly, parameter estimates should be interpretted in context of knowledge of the system and of the sampled data. That is, do these results make sense? What does this mean for maintenance of this system?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References cited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abernethy, R. (2003) _The New Weibull Handbook_. 5th ed. Dr Robert Abernethy.\n",
    "\n",
    "Genschel, U., Meeker, W.Q. (2010) A comparison of Maximum Likelihood and Median-Rank Regression for Weibull Estimation. _Quality Engineering_ **22**: 236-255.\n",
    "\n",
    "Hodkiewicz, M., Ho, M. (2016) Cleaning historical maintenance work order data for reliability analysis. _Journal of Quality in Maintenance Engineering_ **2**(2): 146-163.\n",
    "\n",
    "Ho, M.T. (2016) _A shared reliability database for mobile mining equipment_. Ph.D. thesis, University of Western Australia.\n",
    "\n",
    "Marriott, R. (n.d.) _Reliability Analysis in R_. UWA System Health Lab. https://systemhealthlab.com/research-tools/example-of-a-reliability-analysis-in-r/\n",
    "\n",
    "Meeker, W.Q., Escobar, A. (1998) _Statistical Methods for Reliability Data. Wiley series in probability and statistics. Applied probability and statistics_. John Wiley & Sons, Inc. Canada.\n",
    "\n",
    "O’ Connor, P.T.D., Kleyner, A. (2012) _Practical Reliability Engineering_. 5th ed. John Wiley & Sons, Ltd. West Sussex, United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
